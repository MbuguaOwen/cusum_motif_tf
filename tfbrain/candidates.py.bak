from pathlib import Path
import numpy as np
import pandas as pd
from tqdm import tqdm
from typing import Optional

def detect_candidates(bars: pd.DataFrame, features: pd.DataFrame, cusum_df: pd.DataFrame, cfg: dict, out_dir: Optional[Path] = None) -> pd.DataFrame:
    """Return DataFrame of candidate entries with trigger confirmations and feature snapshot."""
    df = features.copy()
    df["t_index"] = np.arange(len(df))
    df["timestamp"] = bars["timestamp"].values
    # donchian window = micro
    micro = int(cfg["features"]["windows"]["micro"])
    dh = bars["high"].rolling(micro, min_periods=micro).max()
    dl = bars["low"].rolling(micro, min_periods=micro).min()
    atr = df["atr"]
    # blockers telemetry
    block = { "no_cusum":0, "spacing":0, "not_compressed":0, "body_small":0, "accel_low":0,
              "donchpos_low":0, "breakout_buffer":0, "trend_kappa":0, "trend_r2":0, "re_burst":0 }
    # thresholds
    bbw_max = cfg["entry"]["compression"]["bbw_p_max"]
    donch_w_max = cfg["entry"]["compression"]["donch_w_p_max"]
    atr_p_max = cfg["entry"]["compression"]["atr_p_max"]
    body_min = cfg["entry"]["trigger"]["body_frac_min"]
    accel_min = cfg["entry"]["trigger"]["accel_min"]
    donch_pos_min = cfg["entry"]["trigger"]["donch_pos_min"]
    # new trend context + breakout buffer
    trend = cfg.get("entry", {}).get("trend_guard", {})
    kappa_min = float(trend.get("kappa_long_min", float("-inf")))
    r2_min = float(trend.get("slope_r2_min", float("-inf")))
    re_req = bool(trend.get("re_burst_required", False))
    br_atr_min = float(cfg["entry"]["trigger"].get("breakout_atr_min", 0.0))
    spacing = cfg["entry"]["spacing_bars"]
    require_cusum = bool(cfg["entry"]["require_cusum"])
    K = int(cfg["entry"].get("cusum_tolerance_bars", 0))
    # α from train fold will be set later; here we store buffer_atr placeholder (e.g., q75 to be injected upstream)
    alpha_buffer_atr = None  # will be added by caller if needed
    
    burst_idx = set(cusum_df["idx"].tolist()) if cusum_df is not None and len(cusum_df)>0 else set()
    t_bursts = set()
    for b in burst_idx:
        for d in range(-K, K+1):
            t_bursts.add(b+d)
    candidates = []
    last_idx = -10**9
    for i in range(len(df)):
        # spacing
        if i - last_idx < spacing:
            block["spacing"] += 1
            continue
        # require CUSUM at i (or nearby) if enabled
        if require_cusum and (i not in t_bursts):
            block["no_cusum"] += 1
            continue
        # preconditions (compression)
        compressed = ((df["bbw_p"].iloc[i] <= bbw_max/100.0) or (df["donch_w_p"].iloc[i] <= donch_w_max/100.0))
        if not compressed:
            block["not_compressed"] += 1
            continue
        if not (df["atr_p"].iloc[i] <= atr_p_max/100.0):
            block["not_compressed"] += 1
            continue
        # trigger (longs and shorts will be considered separately later; here we just record context)
        c = bars["close"].iloc[i]
        dH = dh.iloc[i]; dL = dl.iloc[i]
        donch_pos = (c - dL) / (dH - dL) if (dH - dL) != 0 else np.nan
        # trend context guards (improves precision)
        kappa_i = np.nan_to_num(df.get("kappa_long", pd.Series([np.nan]*len(df))).iloc[i])
        r2_i = np.nan_to_num(df.get("slope_r2", pd.Series([np.nan]*len(df))).iloc[i])
        if not (kappa_i >= kappa_min):
            block["trend_kappa"] += 1
            continue
        if not (r2_i >= r2_min):
            block["trend_r2"] += 1
            continue
        if re_req and not bool(df.get("re_burst", pd.Series([False]*len(df))).iloc[i]):
            block["re_burst"] += 1
            continue
        # tiny ATR buffer on breakout → quality bump without killing supply
        atr_i = float(atr.iloc[i]) if not np.isnan(atr.iloc[i]) else 0.0
        if br_atr_min > 0 and not np.isnan(dH):
            if not (c >= dH + br_atr_min * atr_i):
                block["breakout_buffer"] += 1
                continue
        if not (df["body_frac"].iloc[i] >= body_min):
            block["body_small"] += 1
            continue
        if not (df["accel"].iloc[i] >= accel_min):
            block["accel_low"] += 1
            continue
        if not (donch_pos >= donch_pos_min):
            block["donchpos_low"] += 1
            continue
        # store
        candidates.append({
            "bar_idx": i,
            "timestamp": int(bars["timestamp"].iloc[i]),
            "close": float(c),
            "atr": float(atr.iloc[i]),
            "donch_high": float(dH) if not np.isnan(dH) else None,
            "donch_low": float(dL) if not np.isnan(dL) else None,
            "donch_pos": float(donch_pos) if not np.isnan(donch_pos) else None,
            "kappa_long": float(df["kappa_long"].iloc[i]) if not np.isnan(df["kappa_long"].iloc[i]) else None,
            "kappa_short": float(df["kappa_short"].iloc[i]) if not np.isnan(df["kappa_short"].iloc[i]) else None,
            "accel": float(df["accel"].iloc[i]) if not np.isnan(df["accel"].iloc[i]) else None,
            "body_frac": float(df["body_frac"].iloc[i]) if not np.isnan(df["body_frac"].iloc[i]) else None,
        })
        last_idx = i
    out = pd.DataFrame(candidates)
    if len(out)==0:
        out = pd.DataFrame(columns=["bar_idx","timestamp","close","atr","donch_high","donch_low","donch_pos","kappa_long","kappa_short","accel","body_frac"])
    # blockers telemetry out
    import json
    try:
        if out_dir is not None:
            (out_dir/"cand_blockers.json").write_text(json.dumps(block, indent=2), encoding="utf-8")
    except Exception:
        pass
    print(f"[cands] blockers: {block}")
    return out
